{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ykV7euL519"
      },
      "source": [
        "## **Setting up a PySpark environment in Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oacfb8Od7TWT",
        "outputId": "8fdab64e-46b1-419d-c026-f6ac5df30bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r            \rHit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [1 InRelease 43.1 kB/88.7 \u001b[0m\u001b[33m\r0% [2 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\u001b[0m\r                                                                               \rHit:3 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [2 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\u001b[0m\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [2 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\u001b[0m\r                                                                               \rHit:5 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [2 InRelease gpgv 15.9 kB] [Connecting to archive.ubuntu.com (91.189.91.39)]\u001b[0m\r                                                                               \rHit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,035 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,554 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [985 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,467 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,332 kB]\n",
            "Fetched 11.6 MB in 2s (5,573 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "27 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.2-bin-hadoop3.2.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\"\n",
        "\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "maF4Rk19L2cE"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "# set up spark session \n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[10]\")\\\n",
        "        .appName(\"twitter Sentiment Analysis\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo0FUZMsMi3x"
      },
      "source": [
        "## **Loading data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5IkTTqi7C8G",
        "outputId": "2fba048a-0733-4032-a893-c7e0a462d921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-22 01:24:58--  http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip [following]\n",
            "--2022-10-22 01:24:58--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81363704 (78M) [application/zip]\n",
            "Saving to: ‘trainingandtestdata.zip’\n",
            "\n",
            "trainingandtestdata 100%[===================>]  77.59M  19.8MB/s    in 4.9s    \n",
            "\n",
            "2022-10-22 01:25:03 (15.7 MB/s) - ‘trainingandtestdata.zip’ saved [81363704/81363704]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "!wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "\n",
        "with zipfile.ZipFile('trainingandtestdata.zip') as zfile:\n",
        "  zfile.extractall('Dataset/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prqoQUeYM5UZ"
      },
      "source": [
        "## **Understanding data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O93uu6-Q9Xya",
        "outputId": "617e45da-5def-4788-9416-a7777387699e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------------------+------------+---------------+--------------------+\n",
            "|sentiment|        id|                date|query_string|           user|                text|\n",
            "+---------+----------+--------------------+------------+---------------+--------------------+\n",
            "|        0|1467810369|Mon Apr 06 22:19:...|    NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
            "|        0|1467810672|Mon Apr 06 22:19:...|    NO_QUERY|  scotthamilton|is upset that he ...|\n",
            "|        0|1467810917|Mon Apr 06 22:19:...|    NO_QUERY|       mattycus|@Kenichan I dived...|\n",
            "|        0|1467811184|Mon Apr 06 22:19:...|    NO_QUERY|        ElleCTF|my whole body fee...|\n",
            "|        0|1467811193|Mon Apr 06 22:19:...|    NO_QUERY|         Karoli|@nationwideclass ...|\n",
            "+---------+----------+--------------------+------------+---------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"sentiment\", IntegerType(), True),\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"query_string\", StringType(), True),\n",
        "    StructField(\"user\", StringType(), True),\n",
        "    StructField(\"text\", StringType(), True)],\n",
        "  )\n",
        "\n",
        "df=spark.read.csv('Dataset/training.1600000.processed.noemoticon.csv',header=False,schema=schema)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDiIOZtvCZMO",
        "outputId": "6df8e09b-8699-4e32-d5d3-66d91f0ed9bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|sentiment|                text|\n",
            "+---------+--------------------+\n",
            "|        0|@switchfoot http:...|\n",
            "|        0|is upset that he ...|\n",
            "|        0|@Kenichan I dived...|\n",
            "|        0|my whole body fee...|\n",
            "|        0|@nationwideclass ...|\n",
            "+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.drop(*('id','date','query_string','user'))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_eor960rcymO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5f21c2-ab73-4b2a-bf08-1b928e01cb4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------+\n",
            "|count(CASE WHEN (isnan(sentiment) OR (sentiment IS NULL)) THEN true END)|\n",
            "+------------------------------------------------------------------------+\n",
            "|                                                                       0|\n",
            "+------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "\n",
        "df.select([count(when(isnan('sentiment') | col('sentiment').isNull() , True))]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5nbsHaJldovH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8334f2d2-1045-42e0-f612-bcc41ef8e211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------+\n",
            "|count(CASE WHEN (isnan(text) OR (text IS NULL)) THEN true END)|\n",
            "+--------------------------------------------------------------+\n",
            "|                                                             0|\n",
            "+--------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "\n",
        "df.select([count(when(isnan('text') | col('text').isNull() , True))]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-1TekOhBo5d",
        "outputId": "ee9ae5a4-9fcd-4cdd-d41f-87373be90937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|sentiment| count|\n",
            "+---------+------+\n",
            "|        0|800000|\n",
            "|        4|800000|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy('sentiment').count().orderBy('count',ascending=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAN7NRh5Cy4c",
        "outputId": "152bac85-a43f-47cf-ff3f-cffce8d654f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(text=\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"),\n",
              " Row(text=\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"),\n",
              " Row(text='@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds'),\n",
              " Row(text='my whole body feels itchy and like its on fire '),\n",
              " Row(text=\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \"),\n",
              " Row(text='@Kwesidei not the whole crew '),\n",
              " Row(text='Need a hug '),\n",
              " Row(text=\"@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\"),\n",
              " Row(text=\"@Tatiana_K nope they didn't have it \"),\n",
              " Row(text='@twittera que me muera ? '),\n",
              " Row(text=\"spring break in plain city... it's snowing \"),\n",
              " Row(text='I just re-pierced my ears '),\n",
              " Row(text=\"@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .\"),\n",
              " Row(text='@octolinz16 It it counts, idk why I did either. you never talk to me anymore '),\n",
              " Row(text=\"@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\"),\n",
              " Row(text='@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!'),\n",
              " Row(text=\"Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\"),\n",
              " Row(text='about to file taxes '),\n",
              " Row(text='@LettyA ahh ive always wanted to see rent  love the soundtrack!!'),\n",
              " Row(text='@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks? ')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.filter(df.sentiment == 0).select(\"text\").take(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKjCro3tEj63",
        "outputId": "b2372fd3-851c-4dea-d911-b21ff1850453"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(text='I LOVE @Health4UandPets u guys r the best!! '),\n",
              " Row(text='im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!'),\n",
              " Row(text='@DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart. '),\n",
              " Row(text='Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup'),\n",
              " Row(text='@LovesBrooklyn2 he has that effect on everyone '),\n",
              " Row(text='@ProductOfFear You can tell him that I just burst out laughing really loud because of that  Thanks for making me come out of my sulk!'),\n",
              " Row(text='@r_keith_hill Thans for your response. Ihad already find this answer '),\n",
              " Row(text=\"@KeepinUpWKris I am so jealous, hope you had a great time in vegas! how did you like the ACM's?! LOVE YOUR SHOW!! \"),\n",
              " Row(text='@tommcfly ah, congrats mr fletcher for finally joining twitter '),\n",
              " Row(text='@e4VoIP I RESPONDED  Stupid cat is helping me type. Forgive errors '),\n",
              " Row(text='crazy day of school. there for 10 hours straiiight. about to watch the hills. @spencerpratt told me too! ha. happy birthday JB! '),\n",
              " Row(text='@naughtyhaughty HOW DID I FORGET ABOUT TWO AND A HALF MEN?!?!? I LOVE THAT SHOW!!! '),\n",
              " Row(text=\"@nileyjileyluver Haha, don't worry! You'll get the hang of it! \"),\n",
              " Row(text=\"@soundwav2010 At least I won't be the only one feeling lost! This may cause me many later than usual nights, already addicting \"),\n",
              " Row(text=\"@LutheranLucciol Make sure you DM me if you post a link to that video! &lt;LOL&gt;So I don't miss it   Better get permission and blessing first?\"),\n",
              " Row(text='Just added tweetie to my new iPhone '),\n",
              " Row(text=\"@michellardi i really don't know. i think its Globe!  yeah! sana gumaling na ko para alam ko na din kung makakasama ako! )\"),\n",
              " Row(text='@nicolerichie: your picture is very sweet '),\n",
              " Row(text=\"Catching Up on Emails, RSS and Random BACN. Then I'm cutting out early tonight (11:30PM) to have Dinner with @lauraw \"),\n",
              " Row(text='Dancing around the room in Pjs, jamming to my ipod. Getting dizzy. Well twitter, you asked! ')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.filter(df.sentiment == 4).select(\"text\").take(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1GvsgViEyNg",
        "outputId": "5875dbfc-9305-4afa-b3b4-12b337746011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+-------------+\n",
            "|sentiment|                text|pre_clean_len|\n",
            "+---------+--------------------+-------------+\n",
            "|        0|@switchfoot http:...|           19|\n",
            "|        0|is upset that he ...|           21|\n",
            "|        0|@Kenichan I dived...|           18|\n",
            "|        0|my whole body fee...|           10|\n",
            "|        0|@nationwideclass ...|           21|\n",
            "+---------+--------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "@udf\n",
        "def len_row(text):\n",
        "  return len(text.split())\n",
        "df = df.withColumn(\"pre_clean_len\", len_row(df.text))\n",
        "df = df.withColumn(\"pre_clean_len\", df.pre_clean_len.cast(IntegerType()))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a3jLPJbbRf0c"
      },
      "outputs": [],
      "source": [
        "# import plotly.express as px\n",
        "\n",
        "# fig = px.box(df.toPandas(), y=\"pre_clean_len\")\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOCYn50-hs_N"
      },
      "source": [
        "## **Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROFC78pGNBR7",
        "outputId": "87d03398-3aac-4145-98b3-13fdf5f157a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from numpy.core.defchararray import isnumeric\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('stopwords')\n",
        "stop_words=list(stopwords.words('english'))\n",
        "stop_words.remove('not')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eDo2U18qqub6"
      },
      "outputs": [],
      "source": [
        "mydict = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\",\"im\":\"i am\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nE6JMDGyNQT0"
      },
      "outputs": [],
      "source": [
        "def replace_dict(text):\n",
        "  text2=\"\"\n",
        "  for word in text.lower().split():\n",
        "    if word in mydict.keys():\n",
        "      text2=text2+\" \"+mydict[word]\n",
        "    else:\n",
        "      text2=text2+\" \"+word\n",
        "  return text2.split()\n",
        "\n",
        "@udf\n",
        "def clean(text):\n",
        "  #Removing urls\n",
        "  text=re.sub(r'http\\S+', '', text)\n",
        "\n",
        "  #Removing html elements\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "  #Removing whitespaces\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  #Removing punctuations\n",
        "  text=text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  #lemmatize\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_text=[lemmatizer.lemmatize(word.lower()) for word in replace_dict(text) if word not in stop_words if not np.char.isnumeric(word)  ]\n",
        "  \n",
        "  return ' '.join(lemmatized_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66ehl7PqNPmQ",
        "outputId": "0ffa4048-1a21-4b8c-a0cb-2b4fe2e46fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+-------------+\n",
            "|sentiment|                text|pre_clean_len|\n",
            "+---------+--------------------+-------------+\n",
            "|        0|switchfoot awww t...|           10|\n",
            "|        0|upset cant update...|           12|\n",
            "|        0|kenichan dived ma...|           10|\n",
            "|        0|whole body feel i...|            6|\n",
            "|        0|nationwideclass n...|            6|\n",
            "+---------+--------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.withColumn(\"text\", clean(df.text))\n",
        "df = df.withColumn(\"pre_clean_len\", len_row(df.text))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "etRTpS2Ue64D"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql.functions import isnan, when, count, col\n",
        "\n",
        "# df.select([count(when(isnan('text') | col('text').isNull() , True))]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R3npqZnCxDyp"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "df2=df.where(df.pre_clean_len>0).select(\"*\").orderBy(F.rand())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6W-rh32b-kL"
      },
      "source": [
        "## **Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "x9vKPh7-orGg"
      },
      "outputs": [],
      "source": [
        "(train_set, val_set, test_set) = df2.randomSplit([0.98, 0.01, 0.01], seed = 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "A4GxHXPiqub8"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import  IDF, Tokenizer, CountVectorizer, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
        "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
        "label_stringIdx = StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
        "lr = LogisticRegression(maxIter=100)\n",
        "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
        "\n",
        "pipelineFit = pipeline.fit(train_set)\n",
        "predictions_train = pipelineFit.transform(train_set)\n",
        "predictions_val = pipelineFit.transform(val_set)\n",
        "predictions_test = pipelineFit.transform(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "accuracy_train = predictions_train.filter(predictions_train.label == predictions_train.prediction).count() / float(train_set.count())\n",
        "roc_auc_train = evaluator.evaluate(predictions_train)\n",
        "\n",
        "accuracy_val = predictions_val.filter(predictions_val.label == predictions_val.prediction).count() / float(val_set.count())\n",
        "roc_auc_val = evaluator.evaluate(predictions_val)\n",
        "\n",
        "accuracy_test = predictions_test.filter(predictions_test.label == predictions_test.prediction).count() / float(test_set.count())\n",
        "roc_auc_test = evaluator.evaluate(predictions_test)\n",
        "\n",
        "print(\"train: Accuracy Score: {0:.4f}\".format(accuracy_train))\n",
        "print(\"train: ROC-AUC: {0:.4f}\".format(roc_auc_train))\n",
        "\n",
        "print(\"val: Accuracy Score: {0:.4f}\".format(accuracy_val))\n",
        "print(\"val: ROC-AUC: {0:.4f}\".format(roc_auc_val))\n",
        "\n",
        "print(\"test: Accuracy Score: {0:.4f}\".format(accuracy_test))\n",
        "print(\"test: ROC-AUC: {0:.4f}\".format(roc_auc_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMoLAMuUH5bK",
        "outputId": "fbb505fe-94ca-4d94-b82c-7b1d8b0904b4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: Accuracy Score: 0.8090\n",
            "train: ROC-AUC: 0.8851\n",
            "val: Accuracy Score: 0.7790\n",
            "val: ROC-AUC: 0.8516\n",
            "test: Accuracy Score: 0.7860\n",
            "test: ROC-AUC: 0.8548\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "720762d3900f746adb074a8e15d7c84fbfcf912d6b26d58b117a910e749e8f3b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}