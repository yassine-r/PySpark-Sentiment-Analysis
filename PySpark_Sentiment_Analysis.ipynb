{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ykV7euL519"
      },
      "source": [
        "## **Setting up a PySpark environment in Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oacfb8Od7TWT",
        "outputId": "1c99bccd-7ce8-48f2-fbc7-01c05ebd4d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,208 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,248 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,035 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,554 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,467 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,332 kB]\n",
            "Fetched 13.1 MB in 4s (2,990 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "29 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.2-bin-hadoop3.2.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop3.2\"\n",
        "\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "maF4Rk19L2cE"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "# set up spark session \n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[10]\")\\\n",
        "        .appName(\"twitter Sentiment Analysis\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo0FUZMsMi3x"
      },
      "source": [
        "## **Loading data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5IkTTqi7C8G",
        "outputId": "d54c1b69-5e65-4f1a-cdc9-4b5bc0f77640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-19 15:58:15--  http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip [following]\n",
            "--2022-10-19 15:58:15--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81363704 (78M) [application/zip]\n",
            "Saving to: ‘trainingandtestdata.zip’\n",
            "\n",
            "trainingandtestdata 100%[===================>]  77.59M  46.7MB/s    in 1.7s    \n",
            "\n",
            "2022-10-19 15:58:17 (46.7 MB/s) - ‘trainingandtestdata.zip’ saved [81363704/81363704]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "!wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "\n",
        "with zipfile.ZipFile('trainingandtestdata.zip') as zfile:\n",
        "  zfile.extractall('Dataset/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prqoQUeYM5UZ"
      },
      "source": [
        "## **Understanding data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O93uu6-Q9Xya",
        "outputId": "4dc0d338-8ee5-4914-d8d2-e1d7f20713fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------------------+------------+---------------+--------------------+\n",
            "|sentiment|        id|                date|query_string|           user|                text|\n",
            "+---------+----------+--------------------+------------+---------------+--------------------+\n",
            "|        0|1467810369|Mon Apr 06 22:19:...|    NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
            "|        0|1467810672|Mon Apr 06 22:19:...|    NO_QUERY|  scotthamilton|is upset that he ...|\n",
            "|        0|1467810917|Mon Apr 06 22:19:...|    NO_QUERY|       mattycus|@Kenichan I dived...|\n",
            "|        0|1467811184|Mon Apr 06 22:19:...|    NO_QUERY|        ElleCTF|my whole body fee...|\n",
            "|        0|1467811193|Mon Apr 06 22:19:...|    NO_QUERY|         Karoli|@nationwideclass ...|\n",
            "+---------+----------+--------------------+------------+---------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"sentiment\", IntegerType(), True),\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"query_string\", StringType(), True),\n",
        "    StructField(\"user\", StringType(), True),\n",
        "    StructField(\"text\", StringType(), True)],\n",
        "  )\n",
        "\n",
        "df=spark.read.csv('Dataset/training.1600000.processed.noemoticon.csv',header=False,schema=schema)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDiIOZtvCZMO",
        "outputId": "1df39c11-67fd-42e9-9829-333aee9a50ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+\n",
            "|sentiment|                text|\n",
            "+---------+--------------------+\n",
            "|        0|@switchfoot http:...|\n",
            "|        0|is upset that he ...|\n",
            "|        0|@Kenichan I dived...|\n",
            "|        0|my whole body fee...|\n",
            "|        0|@nationwideclass ...|\n",
            "+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.drop(*('id','date','query_string','user'))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_eor960rcymO"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql.functions import isnan, when, count, col\n",
        "\n",
        "# df.select([count(when(isnan('sentiment') | col('sentiment').isNull() , True))]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5nbsHaJldovH"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql.functions import isnan, when, count, col\n",
        "\n",
        "# df.select([count(when(isnan('text') | col('text').isNull() , True))]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-1TekOhBo5d",
        "outputId": "97beff70-7cad-4db7-b09b-660ecf89cdea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|sentiment| count|\n",
            "+---------+------+\n",
            "|        0|800000|\n",
            "|        4|800000|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy('sentiment').count().orderBy('count',ascending=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAN7NRh5Cy4c",
        "outputId": "1c624940-6ce3-4aab-c861-8195e0c541e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(text=\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"),\n",
              " Row(text=\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"),\n",
              " Row(text='@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds'),\n",
              " Row(text='my whole body feels itchy and like its on fire '),\n",
              " Row(text=\"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \"),\n",
              " Row(text='@Kwesidei not the whole crew '),\n",
              " Row(text='Need a hug '),\n",
              " Row(text=\"@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\"),\n",
              " Row(text=\"@Tatiana_K nope they didn't have it \"),\n",
              " Row(text='@twittera que me muera ? '),\n",
              " Row(text=\"spring break in plain city... it's snowing \"),\n",
              " Row(text='I just re-pierced my ears '),\n",
              " Row(text=\"@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .\"),\n",
              " Row(text='@octolinz16 It it counts, idk why I did either. you never talk to me anymore '),\n",
              " Row(text=\"@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\"),\n",
              " Row(text='@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!'),\n",
              " Row(text=\"Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\"),\n",
              " Row(text='about to file taxes '),\n",
              " Row(text='@LettyA ahh ive always wanted to see rent  love the soundtrack!!'),\n",
              " Row(text='@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks? ')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.filter(df.sentiment == 0).select(\"text\").take(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKjCro3tEj63",
        "outputId": "66de26f2-686e-4798-eb60-7fc8120e34bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(text='I LOVE @Health4UandPets u guys r the best!! '),\n",
              " Row(text='im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!'),\n",
              " Row(text='@DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart. '),\n",
              " Row(text='Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup'),\n",
              " Row(text='@LovesBrooklyn2 he has that effect on everyone ')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.filter(df.sentiment == 4).select(\"text\").take(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1GvsgViEyNg",
        "outputId": "dc5e0da7-7bf6-4b7b-a82c-e83c7b9c602d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+-------------+\n",
            "|sentiment|                text|pre_clean_len|\n",
            "+---------+--------------------+-------------+\n",
            "|        0|@switchfoot http:...|           19|\n",
            "|        0|is upset that he ...|           21|\n",
            "|        0|@Kenichan I dived...|           18|\n",
            "|        0|my whole body fee...|           10|\n",
            "|        0|@nationwideclass ...|           21|\n",
            "+---------+--------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "@udf\n",
        "def len_row(text):\n",
        "  return len(text.split())\n",
        "df = df.withColumn(\"pre_clean_len\", len_row(df.text))\n",
        "df = df.withColumn(\"pre_clean_len\", df.pre_clean_len.cast(IntegerType()))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a3jLPJbbRf0c"
      },
      "outputs": [],
      "source": [
        "# import plotly.express as px\n",
        "\n",
        "# fig = px.box(df.toPandas(), y=\"pre_clean_len\")\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOCYn50-hs_N"
      },
      "source": [
        "## **Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROFC78pGNBR7",
        "outputId": "6d37a1b7-2dfd-4670-a404-6684b42a8839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from numpy.core.defchararray import isnumeric\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('stopwords')\n",
        "stop_words=list(stopwords.words('english'))\n",
        "stop_words.remove('not')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eDo2U18qqub6"
      },
      "outputs": [],
      "source": [
        "mydict = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\",\"im\":\"i am\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nE6JMDGyNQT0"
      },
      "outputs": [],
      "source": [
        "def replace_dict(text):\n",
        "  text2=\"\"\n",
        "  for word in text.lower().split():\n",
        "    if word in mydict.keys():\n",
        "      text2=text2+\" \"+mydict[word]\n",
        "    else:\n",
        "      text2=text2+\" \"+word\n",
        "  return text2.split()\n",
        "\n",
        "@udf\n",
        "def clean(text):\n",
        "  #Removing urls\n",
        "  text=re.sub(r'http\\S+', '', text)\n",
        "\n",
        "  #Removing html elements\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "  #Removing whitespaces\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  #Removing punctuations\n",
        "  text=text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  #lemmatize\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_text=[lemmatizer.lemmatize(word.lower()) for word in replace_dict(text) if word not in stop_words if not np.char.isnumeric(word)  ]\n",
        "  \n",
        "  return ' '.join(lemmatized_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66ehl7PqNPmQ",
        "outputId": "5c322224-e1af-49f9-f231-8e307b475aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+-------------+\n",
            "|sentiment|                text|pre_clean_len|\n",
            "+---------+--------------------+-------------+\n",
            "|        0|switchfoot awww t...|           10|\n",
            "|        0|upset cant update...|           12|\n",
            "|        0|kenichan dived ma...|           10|\n",
            "|        0|whole body feel i...|            6|\n",
            "|        0|nationwideclass n...|            6|\n",
            "+---------+--------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.withColumn(\"text\", clean(df.text))\n",
        "df = df.withColumn(\"pre_clean_len\", len_row(df.text))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "etRTpS2Ue64D"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql.functions import isnan, when, count, col\n",
        "\n",
        "# df.select([count(when(isnan('text') | col('text').isNull() , True))]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R3npqZnCxDyp"
      },
      "outputs": [],
      "source": [
        "df2=df.where(df.pre_clean_len>0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6W-rh32b-kL"
      },
      "source": [
        "## **Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "x9vKPh7-orGg"
      },
      "outputs": [],
      "source": [
        "(train_set, val_set, test_set) = df2.randomSplit([0.98, 0.01, 0.01], seed = 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kAGTvLoqqub8"
      },
      "outputs": [],
      "source": [
        "#  NB TF-IDF with Logistic Regression is quite strong combination, and showed robust performance as high as Word2Vec + Convolutional Neural Network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4GxHXPiqub8"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import  IDF, Tokenizer, CountVectorizer, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
        "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
        "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "lr = LogisticRegression(maxIter=100)\n",
        "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n",
        "\n",
        "pipelineFit = pipeline.fit(train_set)\n",
        "predictions_train = pipelineFit.transform(train_set)\n",
        "predictions_val = pipelineFit.transform(val_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVXDl89vqub8"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
        "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "Wo0FUZMsMi3x",
        "prqoQUeYM5UZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "720762d3900f746adb074a8e15d7c84fbfcf912d6b26d58b117a910e749e8f3b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}